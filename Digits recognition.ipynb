{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import neural_network\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"Data/train.csv\")\n",
    "n, p = np.shape(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ntrain = 2500\n",
    "ntest = 100\n",
    "train = data.iloc[0:ntrain,1:]/256\n",
    "trainlabel = data.iloc[0:ntrain, 0]\n",
    "test = data.iloc[ntrain:ntest+ntrain,1:]/256\n",
    "testlabel = data.iloc[ntrain:ntest+ntrain,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ytrain = np.zeros((ntrain, 10))\n",
    "ytrain[(np.arange(ntrain),trainlabel.values)] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Codons un neural net \"from scratch\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class nnet:\n",
    "    def __init__(self, sizes):\n",
    "        self.nblayers = len(sizes)\n",
    "        self.poids = [np.random.randn(i, j) for i, j in zip(sizes[0:], sizes[1:])]\n",
    "        self.biais = [np.random.randn(1, i) for i in sizes[1:]]\n",
    "    \n",
    "    def activation(self, x):\n",
    "        return 1/(1+np.exp(-x)) \n",
    "    \n",
    "    def activationPrime(self, x):\n",
    "        a = self.activation(x)\n",
    "        return a*(1-a)\n",
    "    \n",
    "    def gradPerte(self, a, y):\n",
    "        return a-y\n",
    "    \n",
    "    def feedforward(self, x):\n",
    "        a = x\n",
    "        for p,b in zip(self.poids, self.biais):\n",
    "            a = self.activation(np.dot(a, p) + b)\n",
    "        return a\n",
    "        \n",
    "    def backpropagation(self, x, y):\n",
    "        n = len(x)\n",
    "        a = [x.reshape(1,n)]\n",
    "        z = []\n",
    "        y = y.reshape(1,10)\n",
    "        for p,b in zip(self.poids, self.biais):\n",
    "            z.append(np.dot(a[-1], p) + b)\n",
    "            a.append(self.activation(z[-1]))\n",
    "        \n",
    "        delta = [self.gradPerte(a[-1], y)*self.activationPrime(z[-1])]\n",
    "        deriveePoids = [np.dot(a[-2].transpose(), delta[0])]\n",
    "        \n",
    "        for i in range(2, self.nblayers):\n",
    "            delta = [np.dot(delta[0], self.poids[-i+1].transpose())*self.activationPrime(z[-i])] + delta\n",
    "            deriveePoids = [np.dot(a[-i-1].transpose(), delta[0])] + deriveePoids\n",
    "        \n",
    "        return deriveePoids, delta\n",
    "    \n",
    "    def fit(self, x, y, batch_size=10, learning_rate = 3, max_iter = 30):\n",
    "        n = len(x)\n",
    "        indices = np.arange(n)\n",
    "        \n",
    "        for j in range(max_iter):\n",
    "            np.random.shuffle(indices)\n",
    "\n",
    "            for i in range(0, n, batch_size):\n",
    "                sumP = [np.zeros(p.shape) for p in self.poids]\n",
    "                sumB = [np.zeros(b.shape) for b in self.biais]\n",
    "\n",
    "                for ind in indices[i:i+batch_size]:\n",
    "                   \n",
    "                    dp, db = self.backpropagation(x[ind],y[ind])\n",
    "                    \n",
    "                    \n",
    "                    sumP = [p + dp1 for p, dp1 in zip(sumP, dp)]\n",
    "                    sumB = [b + db1 for b, db1 in zip(sumB, db)]\n",
    "                    \n",
    "                    \n",
    "                self.poids = [p - (learning_rate/batch_size)*dp1 for p, dp1 in zip(self.poids, sumP)]\n",
    "                self.biais = [b - (learning_rate/batch_size)*db1 for b, db1 in zip(self.biais, sumB)]\n",
    "                \n",
    "    def predict(self, x):\n",
    "        predictions = []\n",
    "        for elm in x:\n",
    "            predictions.append(np.argmax(self.feedforward(elm), axis=1))\n",
    "        return np.array(predictions)\n",
    "        \n",
    "    def performance(self, x, y):\n",
    "        pred = self.predict(x)\n",
    "        n = len(x)\n",
    "        \n",
    "        return np.sum(y.reshape(1,n) == pred.reshape(1,n))/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = nnet([784,30,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(train.values, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le taux de bonnes prédictions est alors: 89.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Le taux de bonnes prédictions est alors: {}%\".format(net.performance(test.values, testlabel.values)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
